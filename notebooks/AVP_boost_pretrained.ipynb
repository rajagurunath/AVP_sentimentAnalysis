{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AVP_boost_pretrained.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fzbr71y4juoY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3b2de0e8-27c6-40bd-ad86-acaf8ca95111","executionInfo":{"status":"ok","timestamp":1564210932336,"user_tz":-330,"elapsed":2767,"user":{"displayName":"guru nath","photoUrl":"https://lh3.googleusercontent.com/-h5yVgtx8Ghg/AAAAAAAAAAI/AAAAAAAAAHk/jA_YXUJ3mXI/s64/photo.jpg","userId":"08973334926124530429"}}},"source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["gdrive\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tfH6adZXj1Ml","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":445},"outputId":"1713e81a-c432-497f-caae-61c3f9dda9b4","executionInfo":{"status":"ok","timestamp":1564200366984,"user_tz":-330,"elapsed":7452,"user":{"displayName":"guru nath","photoUrl":"https://lh3.googleusercontent.com/-h5yVgtx8Ghg/AAAAAAAAAAI/AAAAAAAAAHk/jA_YXUJ3mXI/s64/photo.jpg","userId":"08973334926124530429"}}},"source":["!pip install pytorch_transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch_transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n","\r\u001b[K     |██▍                             | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.16.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.9.189)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.1.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.6.8)\n","Collecting sentencepiece (from pytorch_transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 25.7MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.12.197)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n","Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch_transformers) (0.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch_transformers) (2.5.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.189->boto3->pytorch_transformers) (1.12.0)\n","Installing collected packages: sentencepiece, pytorch-transformers\n","Successfully installed pytorch-transformers-1.0.0 sentencepiece-0.1.82\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NpJBdVeQ1SaC","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/AVP\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zl-Dt1EVtoAz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"b54637f0-a4d4-487c-9994-1cd3b77dd888","executionInfo":{"status":"ok","timestamp":1564204296437,"user_tz":-330,"elapsed":1030,"user":{"displayName":"guru nath","photoUrl":"https://lh3.googleusercontent.com/-h5yVgtx8Ghg/AAAAAAAAAAI/AAAAAAAAAHk/jA_YXUJ3mXI/s64/photo.jpg","userId":"08973334926124530429"}}},"source":["import torch\n","from pytorch_transformers import *\n","import pandas as pd\n","# PyTorch-Transformers has a unified API\n","# for 6 transformer architectures and 27 pretrained weights.\n","#          Model          | Tokenizer          | Pretrained weights shortcut\n","MODELS = [(BertModel,       BertTokenizer,      'bert-base-uncased'),\n","          (OpenAIGPTModel,  OpenAIGPTTokenizer, 'openai-gpt'),\n","          (GPT2Model,       GPT2Tokenizer,      'gpt2'),\n","          (TransfoXLModel,  TransfoXLTokenizer, 'transfo-xl-wt103'),\n","          (XLNetModel,      XLNetTokenizer,     'xlnet-base-cased'),\n","          (XLMModel,        XLMTokenizer,       'xlm-mlm-enfr-1024')]\n","MODELS[0]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(pytorch_transformers.modeling_bert.BertModel,\n"," pytorch_transformers.tokenization_bert.BertTokenizer,\n"," 'bert-base-uncased')"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"W-MmpvpZj1Pr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"8e6d5d1b-6737-4990-8d7a-b4c25a6e4d04","executionInfo":{"status":"ok","timestamp":1564202300930,"user_tz":-330,"elapsed":127984,"user":{"displayName":"guru nath","photoUrl":"https://lh3.googleusercontent.com/-h5yVgtx8Ghg/AAAAAAAAAAI/AAAAAAAAAHk/jA_YXUJ3mXI/s64/photo.jpg","userId":"08973334926124530429"}}},"source":["\n","# Let's encode some text in a sequence of hidden-states using each model:\n","for model_class, tokenizer_class, pretrained_weights in MODELS:\n","    # Load pretrained model/tokenizer\n","    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","    model = model_class.from_pretrained(pretrained_weights)\n","\n","    # Encode text\n","    input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\")])\n","    with torch.no_grad():\n","        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 2636385.17B/s]\n","100%|██████████| 313/313 [00:00<00:00, 84774.45B/s]\n","100%|██████████| 440473133/440473133 [00:08<00:00, 52590568.61B/s]\n","100%|██████████| 815973/815973 [00:00<00:00, 5530597.51B/s]\n","100%|██████████| 458495/458495 [00:00<00:00, 3768459.34B/s]\n","100%|██████████| 273/273 [00:00<00:00, 181753.17B/s]\n","100%|██████████| 478750579/478750579 [00:10<00:00, 47014939.66B/s]\n","100%|██████████| 1042301/1042301 [00:00<00:00, 5926592.50B/s]\n","100%|██████████| 456318/456318 [00:00<00:00, 3137271.52B/s]\n","100%|██████████| 176/176 [00:00<00:00, 92552.35B/s]\n","100%|██████████| 548118077/548118077 [00:10<00:00, 53481416.81B/s]\n","100%|██████████| 9143613/9143613 [00:00<00:00, 26632721.68B/s]\n","100%|██████████| 606/606 [00:00<00:00, 159008.33B/s]\n","100%|██████████| 1140884800/1140884800 [00:26<00:00, 43621425.08B/s]\n","100%|██████████| 798011/798011 [00:00<00:00, 4524007.78B/s]\n","100%|██████████| 641/641 [00:00<00:00, 151621.30B/s]\n","100%|██████████| 467042463/467042463 [00:08<00:00, 57992056.11B/s]\n","100%|██████████| 1452741/1452741 [00:00<00:00, 7782797.01B/s]\n","100%|██████████| 1008321/1008321 [00:00<00:00, 5660396.41B/s]\n","100%|██████████| 396/396 [00:00<00:00, 87289.49B/s]\n","100%|██████████| 830121242/830121242 [00:15<00:00, 52782597.90B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0z3kPAdS0XC6","colab_type":"code","colab":{}},"source":["def give_feature(model:tuple,df:pd.DataFrame):\n","    tokenizer = model[1].from_pretrained('bert-large-cased')\n","    model = model[0].from_pretrained('bert-large-cased')\n","    \n","    def return_feat(x):\n","      input_ids = torch.tensor([tokenizer.encode(x)])\n","      with torch.no_grad():\n","          last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n","      return last_hidden_states\n","    feat=[]\n","    for _,row in df.iterrows():\n","      feat.append(return_feat(row['text']))\n","    return torch.tensor(feat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"imBnhjxn1M-4","colab_type":"code","colab":{}},"source":["df=pd.read_csv('train_F3WbcTw.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDb2o3xG1d-D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a3f35e5b-4f82-47dd-aca5-553ae160cb4b","executionInfo":{"status":"error","timestamp":1564209608284,"user_tz":-330,"elapsed":80633,"user":{"displayName":"guru nath","photoUrl":"https://lh3.googleusercontent.com/-h5yVgtx8Ghg/AAAAAAAAAAI/AAAAAAAAAHk/jA_YXUJ3mXI/s64/photo.jpg","userId":"08973334926124530429"}}},"source":["resFeat=give_feature(MODELS[0],df)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n","100%|██████████| 213450/213450 [00:00<00:00, 2428283.66B/s]\n","100%|██████████| 521/521 [00:00<00:00, 117910.34B/s]\n","100%|██████████| 1338740706/1338740706 [00:29<00:00, 45217265.48B/s]\n","Token indices sequence length is longer than the specified maximum sequence length for this model (2904 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-7a8f9b96aca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresFeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgive_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-41a82c82147b>\u001b[0m in \u001b[0;36mgive_feature\u001b[0;34m(model, df)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-41a82c82147b>\u001b[0m in \u001b[0;36mreturn_feat\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Models outputs are now tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    708\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m         return F.embedding(\n\u001b[1;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:193"]}]},{"cell_type":"code","metadata":{"id":"JRTDGftDj1Sg","colab_type":"code","colab":{}},"source":["# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n","BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n","                      BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,\n","                      BertForQuestionAnswering]\n","\n","# All the classes for an architecture can be initiated from pretrained weights for this architecture\n","# Note that additional weights added for fine-tuning are only initialized\n","# and need to be trained on the down-stream task\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","for model_class in BERT_MODEL_CLASSES:\n","    # Load pretrained model/tokenizer\n","    model = model_class.from_pretrained('bert-base-uncased')\n","\n","# Models can return full list of hidden-states & attentions weights at each layer\n","model = model_class.from_pretrained(pretrained_weights,\n","                                    output_hidden_states=True,\n","                                    output_attentions=True)\n","input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n","all_hidden_states, all_attentions = model(input_ids)[-2:]\n","\n","# Models are compatible with Torchscript\n","model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n","traced_model = torch.jit.trace(model, (input_ids,))\n","\n","# Simple serialization for models and tokenizers\n","model.save_pretrained('./directory/to/save/')  # save\n","model = model_class.from_pretrained('./directory/to/save/')  # re-load\n","tokenizer.save_pretrained('./directory/to/save/')  # save\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUUB6kluj1Vt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":164},"outputId":"62dfbc10-ee7e-4921-e23f-3476ecbe727d","executionInfo":{"status":"error","timestamp":1564216631400,"user_tz":-330,"elapsed":1051,"user":{"displayName":"guru nath","photoUrl":"https://lh3.googleusercontent.com/-h5yVgtx8Ghg/AAAAAAAAAAI/AAAAAAAAAHk/jA_YXUJ3mXI/s64/photo.jpg","userId":"08973334926124530429"}}},"source":["train.text.apply(lambda x:len(x))"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b1b1975ae74a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"OflKfSH_j1Y8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bV_NbxR4j1KO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}